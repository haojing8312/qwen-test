{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf360e4-b4c1-48e8-a0af-2d2bae2b948b",
   "metadata": {},
   "source": [
    "# <center>零门槛部署Qwen开源大模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4015997-5e87-409f-90cf-382b1e9fba59",
   "metadata": {},
   "source": [
    "- 实验内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6ba6f7-b23a-4e1b-a435-5a41c4743fdc",
   "metadata": {},
   "source": [
    "1.模型部署"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7686d28-ade0-48e0-b0e6-bb937990549a",
   "metadata": {},
   "source": [
    "- 限时免费算力获取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb4275-76e9-4cd6-a2f2-6fd4b0916505",
   "metadata": {},
   "source": [
    "&emsp;&emsp;登录魔搭社区：https://www.modelscope.cn/home ，点击注册："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba98984a-9955-4135-8d19-8ca7c069ae14",
   "metadata": {},
   "source": [
    "<img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171441920.png\" alt=\"image-20240417144137477\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02a7109-3549-4427-848a-e1d477d42919",
   "metadata": {},
   "source": [
    "输入账号密码完成注册："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed78061-5677-4858-b148-ed6301f0d65d",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171443072.png\" alt=\"image-20240417144344970\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dbfe90-ba5c-455a-9c4f-0842e584be97",
   "metadata": {},
   "source": [
    "注册完成后，点击个人中心，点击绑定阿里云账号："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a119d7-5381-4764-bf35-1787df3e8c22",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171444204.png\" alt=\"image-20240417144458827\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fbbfae-5cb2-4420-b871-ea05a70a0930",
   "metadata": {},
   "source": [
    "在跳转页面中选择登录阿里云，未注册阿里云也可以在当前页面注册："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e71d236-aaa6-4e34-9ec9-bc4e08f451fd",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171447094.png\" alt=\"image-20240417144756962\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a62ad3-acea-4bc4-b203-3f54ce51b877",
   "metadata": {},
   "source": [
    "点击授权："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804903e3-0ec0-4601-8fe3-8fc97dfc95a9",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171448989.png\" alt=\"image-20240417144828784\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7704da4a-2dd7-4fcf-ac13-9b0e9a5dcbe9",
   "metadata": {},
   "source": [
    "绑定完成后，点击左侧“我的Notebook”，即可查看当前账号获赠算力情况。对于首次绑定阿里云账号的用户，都会赠送永久免费的CPU环境（8核32G内存）和36小时限时使用的GPU算力（32G内存+24G显存）。这里的GPU算力会根据实际使用情况扣除剩余时间，总共36小时的使用时间完全足够进行前期各项实验。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b9abd1-8966-4588-9981-f7f3062bea8b",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171509070.png\" alt=\"image-20240417150915758\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae54d28e-2058-4b67-b557-17c00b7cd32c",
   "metadata": {},
   "source": [
    "接下来启动GPU在线算力环境，选择方式二、点击启动："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2bf1a-d2d8-406a-913e-cb6dc537aaef",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171509570.png\" alt=\"image-20240417150937257\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67698a17-7678-473b-90e7-62d9c54d0742",
   "metadata": {},
   "source": [
    "稍等片刻即可完成启动，并点击查看Notebook："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a03c25-fd0f-4d03-afab-9a5b05374ebc",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171510775.png\" alt=\"image-20240417151001464\" style=\"zoom: 33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d51b71-f50b-4d40-8d17-c52de3cb2767",
   "metadata": {},
   "source": [
    "即可接入在线NoteBook编程环境："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a05c6c-8915-478d-a63e-76e8d19fed98",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171511445.png\" alt=\"image-20240417151156357\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7217a9-1bb1-41fd-8f4a-477fd4d32dc3",
   "metadata": {},
   "source": [
    "当前NoteBook编程环境和Colab类似（谷歌提供的在线编程环境），可以直接调用在线算力来完成编程工作，并且由于该服务由ModelScope提供，因此当前NoteBook已经完成了CUDA、PyTorch、Tensorflow环境配置，并且已经预安装了大模型部署所需各种库，如Transformer库、vLLM库、modelscope库等，并且当前NoteBook运行环境是Ubuntu操作系统，我们可以通过Jupyter中的Terminal功能对Ubuntu系统进行操作："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad682e93-00f5-4a1d-829f-d4ccf20b1f67",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171516840.png\" alt=\"image-20240417151622754\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b92a4f-ff99-4c51-8702-e6e6677d626d",
   "metadata": {},
   "source": [
    "进入到命令行界面："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c6808d-806c-493b-86cb-c15ace0d9be3",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171516374.png\" alt=\"image-20240417151651288\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ce6467-6dce-4c92-8ebe-f6b90218b82a",
   "metadata": {},
   "source": [
    "输入nvidia-smi，查看当前GPU情况："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82ce593-181a-4053-8ed1-cdd7910de7e3",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171518227.png\" alt=\"image-20240417151818137\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4112700a-3166-45c3-b8a8-11ee74b4bc50",
   "metadata": {},
   "source": [
    "该功能也是我们操作远程Ubuntu系统的核心功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c877e5d-0a6a-437a-b029-6b449c873b13",
   "metadata": {},
   "source": [
    "此外，ModelScope NoteBooko还可以一键拉取ModelScope上发布的模型或项目，直接在云端环境进行运行和实验。这个点击+号开启新的导航页："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7f3ce4-3501-455a-9e33-cee55e90264e",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171522446.png\" alt=\"image-20240417152200353\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dad9b71-c01a-4bed-afb5-580c6ec277b9",
   "metadata": {},
   "source": [
    "并在导航页下方点击模型库："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee695db6-db35-45f4-b1f5-2b0640982fc6",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171522766.png\" alt=\"image-20240417152249678\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68239fcc-1479-4c0a-aec7-de9d4e12598d",
   "metadata": {},
   "source": [
    "即可选择任意模型文档，进行尝试运行："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeca0b80-8157-45ef-90b2-d880b56a3e87",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171524702.png\" alt=\"image-20240417152414553\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932272fc-6a7d-4539-8c63-2c55f2a935c2",
   "metadata": {},
   "source": [
    "例如我们点击选择CodeQwen1.5-7B-Chat，一个基于Qwen1.5-7B微调得到的代码模型。点击即可获得一个新的Jupyter文件，包含了该模型的说明文档和运行代码（也就是该模型在ModelScope上的readme文档）："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a97c8-a0ab-4c89-a06e-cc12da1dbef3",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171526243.png\" alt=\"image-20240417152658151\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a58a14-4154-4636-b02d-c4528b3c4dd6",
   "metadata": {},
   "source": [
    "而如果想要下载某个Jupyter文件到本地，只需要选择文件点击右键、选择Download，即可通过浏览器将项目文件下载到本地："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00354fcf-06d7-4f0b-b864-87c7906073eb",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171530520.png\" alt=\"image-20240417153004426\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c02c158-247c-4234-a21d-8746ee54e13e",
   "metadata": {},
   "source": [
    "当然，这里需要注意的是，哪怕当前在线编程环境已经做了适配，但并不一定满足所有ModelScope中模型运行要求，既并非每个拉取的Jupyter文件都可以直接运行。当前体验课只把ModelScope视作在线编程环境，并不会直接Copy项目文件代码进行运行。不过无论如何，ModelScope Notebook还是为初学者提供了非常友好的、零基础即可入手尝试部署大模型的绝佳实践环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f35082d-0c74-4f39-ac8d-3917d7b25914",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们就借助ModelScope Notebook来完成体验课的大模型部署调用入门实验。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5075be-7879-4753-9009-b3586488cf86",
   "metadata": {},
   "source": [
    "- Qwen1.5-MoE-A2.7B 模型获取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47439d7e-3215-49c3-8a4f-d5fdec508624",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来按照此前规划，我们尝试来部署最新版千问MoE大模型：Qwen1.5-MoE-A2.7B模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8979b70e-3910-4c8a-8a60-914caf5b8ef0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在安装部署任何一个大模型之前，我们都必须找到模型发布平台。对于阿里Qwen大模型，首发平台都是阿里系的ModelScope，我们可以直接在ModelScope看到模型的相关信息。这里我们继续在ModelScope页面，点击上方菜单栏“模型库”，然后再搜索框内输入Qwen1.5-MoE，获得搜索结果如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb3deb2-abc3-4d97-b5ea-246a0aa1558a",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171551960.png\" alt=\"image-20240417155126754\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeb2358-0dc3-40a5-bb4b-942cf5916ff6",
   "metadata": {},
   "source": [
    "点击通义千问1.5-MoE-A2.7B页面，即可查看当前模型基本情况："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3931970b-7359-49da-b58b-3c55946d5d58",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171553011.png\" alt=\"image-20240417155328733\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb68740e-042b-4f0c-8e16-59dad0c43d97",
   "metadata": {},
   "source": [
    "这里点击模型文件，即可查看并下载当前模型文件："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7fd508-f5e0-4a0e-8f3f-6e74600f0572",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171554117.png\" alt=\"image-20240417155443839\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264acfb5-2297-4ec8-8d86-328c258d779e",
   "metadata": {},
   "source": [
    "不过ModelScope只支持通过git或者SDK（modelscope库）进行下载，无法直接通过网页下载各文件："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fbe7a-6547-4a3e-9398-6dca9c792adc",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171555090.png\" alt=\"image-20240417155533878\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b47b673-e174-4e5a-81bd-5e258860c46f",
   "metadata": {},
   "source": [
    "> 稍后我们会进一步介绍如何进行下载及解释各模型文件所代表的含义，这里我们先继续梳理可以进行模型下载的各平台。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8872c47-6064-49ff-8b78-94605f85f9dd",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而除了Qwen1.5-MoE-A2.7B模型外，本次开源的MoE模型开包括Qwen1.5-MoE-A2.7B-chat模型和Qwen1.5-MoE-A2.7B-GPTQ-Int4模型，总共三个模型。根据名称不难发现三个模型定位，Qwen1.5-MoE-A2.7B属于基础模型，Qwen1.5-MoE-A2.7B-chat为对话功能微调模型，更适合执行对话类任务，而Qwen1.5-MoE-A2.7B-GPTQ-Int4则是对话模型的INT4量化版，即参数位只保留4位，运行所需算力和功能性都有所下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918fc932-5dac-4322-81b8-b3262af2fb24",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171559336.png\" alt=\"image-20240417155949286\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d717376-a465-4ade-8e80-7e211b96f4fa",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171600730.png\" alt=\"image-20240417160004679\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da52aa02-04f0-4c38-a9c6-933ba6d89347",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然除了ModelScope，开源大模型还会在Github和HuggingFace上发布。我们可以在Qwen1.5系列模型的Github山主页看到目前MoE系列模型开源情况：https://github.com/QwenLM/Qwen1.5 ："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231fc48a-9f77-42a0-80c4-3824e047461d",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171604573.png\" alt=\"image-20240417160425443\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6677761c-f9c9-4d65-9c5b-6743466c9b08",
   "metadata": {},
   "source": [
    "以及在huggingface的Qwen模型项目主页上，查看当前Qwen系列模型发布情况：https://huggingface.co/Qwen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7c217a-16ba-44f4-afa1-680e7b5fc82c",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171605842.png\" alt=\"image-20240417160511635\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3a1e16-92cd-4423-ba2a-c1d99c66b625",
   "metadata": {},
   "source": [
    "> 目前huggingface访问需要科学上网方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1523a0b5-e583-4577-83e9-6e6b30c87d41",
   "metadata": {},
   "source": [
    "就huggingface和Github来说，Github的功能更偏向代码托管和项目管理，而huggingface和ModelScope类似，都是深度学习、大模型的模型库，模型发布门槛较低，并且huggingface还负责托管了奠定整个大模型基础的transformer库，后续的开源大模型部署和使用也将用到transformer库。因此，整体来看，Github更适合查阅模型各类深度详细信息，而huggingface和ModelScope则更加适合模型关键信息检索与模型快速部署。并且对于huggingface而言，发布的各项模型都可以直接在网页端下载项目文件。例如对于Qwen/Qwen1.5-MoE-A2.7B模型，模型主页https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B 如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1444aa09-053e-4a40-a278-d73270751387",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171615647.png\" alt=\"image-20240417161554542\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c63614-cf9c-4222-811b-601ca22a83da",
   "metadata": {},
   "source": [
    "点击文件和版本说明，即可进入到模型文件页面。而和ModelScope不同的是，huggingface提供了每个项目文件的基于网页端的下载选项，有必要的话我们可以直接从网页端下载项目文件："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e800c25c-3abd-4588-9eb2-3194fa1f9c92",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171617136.png\" alt=\"image-20240417161712028\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6edd9db-d3c0-406e-b37a-82e8435139da",
   "metadata": {},
   "source": [
    "> 除了这些通用的平台外，目前一些大模型部署工具如ollama等，也可以直接下载模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14424328-eb67-4445-b4f0-387ca715820b",
   "metadata": {},
   "source": [
    "- Qwen大模型项目文件介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ad10d-14d9-4f7a-9795-d9705671a198",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而在正式部署模型之前，我们还需要围绕当前Qwen模型的项目文件进行简要介绍。以帮助大家更好的理解一个开源大模型项目的基本构成。这里我们以Qwen为例进行介绍，其他大模型的项目文件构成都基本类似。后续我们在调用任何一个开源大模型之前，都必须完整的项目文件下载到指定目录中，无论是手动下载还是通过某些工具进行下载，都是如此。Qwen模型项目文件构成如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8734d941-87f9-4afd-922c-8a00a0e6d8d9",
   "metadata": {},
   "source": [
    "|文件名|文件功能解释|\n",
    "|:--:|:--|\n",
    "|.gitattributes|项目中用于定义项目特定的属性配置。它可以用来控制不同文件的行为，例如指定文件的对比方式、定义文件的编码、处理换行符等。此文件允许你为特定文件或目录设置一些规则，这些规则会影响Git的各种操作，如合并和检出等。|\n",
    "|LICENSE|文件通常是用来说明模型或软件项目的使用许可。这个文件详细描述了其他用户和开发者可以如何使用、修改、共享和分发这个项目或模型。它规定了法律条款，确保了作者的知识产权同时允许社区以特定方式利用这个资源。|\n",
    "|config.json|通常用于存储软件或模型的基本配置参数，如模型的架构设置、超参数或环境依赖性设置。这些配置使得用户或开发者可以在不修改代码的情况下调整程序行为。|\n",
    "|configuration.json|文件通常也用于类似目的，存储配置信息。具体内容可能包括更详细或专门的设置，如API密钥、用户权限等。|\n",
    "|generation_config.json| 通常用于存储与文本生成相关的配置设置。在涉及到语言模型或文本生成任务的项目中，这个文件可以包含用于文本生成的各种参数，如最大生成长度、温度（用于控制生成文本的随机性）、重复率控制和特定的前置或后置处理规则。|\n",
    "|merges.txt|在自然语言处理领域，特别是在使用基于 Transformer 的模型如 BERT 或 GPT 时，merges.txt 文件常用于存储用于字节对编码（Byte Pair Encoding, BPE）或其他子词分割算法的合并规则。这些规则帮助分词器决定如何将未知或罕见词汇分解成已知的子单位。|\n",
    "|model-0000X-of-00008.safetensors|表示这是模型文件的一部分，其中使用了分片存储的方式。这种命名模式表明文件是分段保存的大型模型的一部分。safetensors 是一种文件格式，用于安全地存储包含模型权重的张量数据。当模型太大而无法单个文件存储时，会将其分成多个部分，每个文件包含模型的一部分数据。这些文件通常在加载模型时需要被一起读取来完整地重建模型。|\n",
    "|model.safetensors.index.json|文件通常用作存储有关模型分片的索引信息。这个索引文件包含了各个分片文件的元数据，比如每个分片的名称、大小、顺序等，以便在需要时正确地加载和重建整个模型。|\n",
    "|tokenizer.json|文件通常包含分词器的配置和词汇表，用于加载和使用分词器进行文本处理。它可能包括词汇的具体列表、分词规则和特殊标记等信息。|\n",
    "|tokenizer_config.json|包含关于分词器设置的配置信息，如是否添加特殊标记到序列的开头或结尾、分词的最大长度等。这个文件用于详细指定如何初始化分词器的各项参数，以确保在处理文本时能够正确应用这些设置。|\n",
    "|vocab.json|文件通常用于存储分词器的词汇表。这个文件包含了模型可以识别的所有词汇的索引，每个词汇或标记都与一个唯一的数字ID关联。|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9bc9e1-5fe5-4ba0-9e52-f9b79c3ff0ed",
   "metadata": {},
   "source": [
    "而我们所说的开源大模型部署与调用，本质上就是下载项目文件并进行运行的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17fed6-42da-44df-9fd6-2fd76edf2108",
   "metadata": {},
   "source": [
    "- Qwen大模型部署与调用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3536090-ae0a-40b3-aa93-d59ec38ef4ee",
   "metadata": {},
   "source": [
    "&emsp;&emsp;正如此前所说，我们需要先下载模型，然后再进行使用。这里受限于单卡24显存限制条件，我们考虑部署Qwen1.5-MoE的INT4版本模型进行使用，即考虑安装部署使用Qwen1.5-MoE-A2.7B-GPTQ-Int4模型，该安装部署方法也可以迁移到其他模型部署中，而该模型也是目前最高效（算力要求较低、同时性能较好）的大模型。同时，考虑到后续微调实验，我们还将介绍Qwen1.5-1.8B模型部署方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eba276-105d-463a-9a3f-48af7c2aff3e",
   "metadata": {},
   "source": [
    "> 根据实际测试，Qwen1.5-MoE-A2.7B和Qwen1.5-MoE-A2.7B-chat模型运行需要28G显存，而Qwen1.5-MoE-A2.7B-GPTQ-Int4则需要10G内存即可调用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8444b841-6bd7-4f0c-bdd5-aa70a1e4da55",
   "metadata": {},
   "source": [
    "> 在[《大模型技术实战课》（第3期）](https://appZe9inzwc2314.h5.xiaoeknow.com)课程中，还会详细介绍多卡环境中部署更高参数模型方法。在实际大模型开发场景下，多卡部署是更为通用的技术需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ece268-b620-411f-aade-3a3eccc369bc",
   "metadata": {},
   "source": [
    "- 开源大模型部署方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d49216-b6bc-4509-b48e-102b33aafc41",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171645681.png\" alt=\"开源大模型下载方法\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdfa216-17c8-459e-8127-8d9f84062626",
   "metadata": {},
   "source": [
    "对于国内用户来说，一个比较稳定的下载和部署开源大模型的方法就是使用ModelScope的SDK进行下载，然后再Transformer库进行调用。接下来我们就照此流程来尝试安装部署Qwen1.5-MoE-A2.7B-GPTQ-Int4模型，其他模型部署方法也类似："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed62e04-b74a-4973-b755-6e6bfa61845c",
   "metadata": {},
   "source": [
    "- Step 1.检查环境依赖是否满足条件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39a14f3-908a-4f4d-ba71-443cbc187196",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于本地开源大模型部署来说，前期配置大模型运行环境是一件较为复杂的工作，需要提前准备好Linux操作系统，CUDA软件、PyTorch框架，以及一系列库、如Transformer库等，并且还需要考虑其中各组件的适配条件是否满足。而现在的ModelScope在线NoteBook环境已经配置好，基本环境都已经满足，唯一需要注意的是，由于Qwen1.5-MoE模型是最新发布的模型，因此需要更新transformers库（4.37以上）才可使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3854ecc-4b54-4302-81d3-bf8a4581a713",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里我们可以在Jupyter进入命令行，然后输入以下命令即可完成更新。由于ModelScope在线环境较为特殊，因此在更新transformer库时会显示部分其他库冲突，这些库并版本不匹配问题并不影响模型调用和运行，忽视即可： "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477476f4-820a-466b-8229-8a04cbc22952",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install git+https://github.com/huggingface/transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf12f0c6-7e49-43c1-86c9-f9ee5e930760",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171659344.png\" alt=\"image-20240417165923243\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "764ecda3-739a-433c-aa5d-60911537f23e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T23:58:34.895608Z",
     "iopub.status.busy": "2024-04-18T23:58:34.895288Z",
     "iopub.status.idle": "2024-04-18T23:58:36.300567Z",
     "shell.execute_reply": "2024-04-18T23:58:36.300023Z",
     "shell.execute_reply.started": "2024-04-18T23:58:34.895587Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16bfddae-8468-4a66-a663-9249eaa7fb9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T23:58:37.911646Z",
     "iopub.status.busy": "2024-04-18T23:58:37.911234Z",
     "iopub.status.idle": "2024-04-18T23:58:37.916988Z",
     "shell.execute_reply": "2024-04-18T23:58:37.916424Z",
     "shell.execute_reply.started": "2024-04-18T23:58:37.911622Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.38.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642bbfc9-671c-4e1b-bcf2-701ff389b874",
   "metadata": {},
   "source": [
    "- Step 2.使用snapshot_download获取模型项目文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a82ac7-282c-424f-90e0-e6843a566e61",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来回到Jupyter页面中执行以下命令完成模型下载："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93fd05a0-2119-4bc4-9fd9-09ff0a772a9c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-18T23:58:40.241131Z",
     "iopub.status.busy": "2024-04-18T23:58:40.240707Z",
     "iopub.status.idle": "2024-04-18T23:58:42.010396Z",
     "shell.execute_reply": "2024-04-18T23:58:42.009901Z",
     "shell.execute_reply.started": "2024-04-18T23:58:40.241106Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 07:58:40,836 - modelscope - INFO - PyTorch version 2.1.2+cu121 Found.\n",
      "2024-04-19 07:58:40,839 - modelscope - INFO - TensorFlow version 2.14.0 Found.\n",
      "2024-04-19 07:58:40,839 - modelscope - INFO - Loading ast index from /mnt/workspace/.cache/modelscope/ast_indexer\n",
      "2024-04-19 07:58:40,839 - modelscope - INFO - No valid ast index found from /mnt/workspace/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n",
      "2024-04-19 07:58:40,892 - modelscope - INFO - Loading done! Current index file version is 1.13.3, with md5 55e7043102d017111a56be6e6d7a6a16 and a total number of 972 components indexed\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbcf131f-1fd7-4e60-b2f6-9201fd3e3c9e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-18T23:58:44.529235Z",
     "iopub.status.busy": "2024-04-18T23:58:44.528799Z",
     "iopub.status.idle": "2024-04-18T23:59:04.325443Z",
     "shell.execute_reply": "2024-04-18T23:59:04.324844Z",
     "shell.execute_reply.started": "2024-04-18T23:58:44.529214Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 662/662 [00:00<00:00, 4.59MB/s]\n",
      "Downloading: 100%|██████████| 51.0/51.0 [00:00<00:00, 408kB/s]\n",
      "Downloading: 100%|██████████| 206/206 [00:00<00:00, 1.40MB/s]\n",
      "Downloading: 100%|██████████| 7.11k/7.11k [00:00<00:00, 24.7MB/s]\n",
      "Downloading: 100%|██████████| 1.59M/1.59M [00:00<00:00, 31.0MB/s]\n",
      "Downloading: 100%|█████████▉| 3.42G/3.42G [00:11<00:00, 326MB/s]\n",
      "Downloading: 100%|██████████| 4.15k/4.15k [00:00<00:00, 12.4MB/s]\n",
      "Downloading: 100%|██████████| 6.70M/6.70M [00:00<00:00, 97.6MB/s]\n",
      "Downloading: 100%|██████████| 1.26k/1.26k [00:00<00:00, 9.43MB/s]\n",
      "Downloading: 100%|██████████| 2.65M/2.65M [00:00<00:00, 86.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "model_dir=snapshot_download(\"qwen/Qwen1.5-1.8B-Chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec75447-92da-4ee9-a5f2-027722f0321f",
   "metadata": {},
   "source": [
    "下载完成后，model_dir变量就代表着当前模型下载地址。在之后的模型调用过程中，模型地址就是模型对象的重要指代变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0830737-82ce-4ea2-a042-008d3d8db929",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T23:59:08.488032Z",
     "iopub.status.busy": "2024-04-18T23:59:08.487678Z",
     "iopub.status.idle": "2024-04-18T23:59:08.491428Z",
     "shell.execute_reply": "2024-04-18T23:59:08.490986Z",
     "shell.execute_reply.started": "2024-04-18T23:59:08.487996Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/workspace/.cache/modelscope/qwen/Qwen1___5-1___8B-Chat'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862cc96b-801b-4a62-9128-bf4cef9afdba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-17T09:06:20.542416Z",
     "iopub.status.busy": "2024-04-17T09:06:20.542101Z",
     "iopub.status.idle": "2024-04-17T09:06:20.546052Z",
     "shell.execute_reply": "2024-04-17T09:06:20.545353Z",
     "shell.execute_reply.started": "2024-04-17T09:06:20.542395Z"
    }
   },
   "source": [
    "而我们也可以据此查阅模型下载情况。回到命令行界面中，使用命令      \n",
    "```bash\n",
    "cd /mnt/workspace/.cache/modelscope/qwen/\n",
    "ll\n",
    "```      \n",
    "查看下载文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e35c9-4f9e-454e-8be9-d342ec03eba4",
   "metadata": {},
   "source": [
    "- Step 3.使用transformer库调用本地大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ce38541-77a3-4924-91e6-042ee809f02f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-18T23:59:34.615285Z",
     "iopub.status.busy": "2024-04-18T23:59:34.614969Z",
     "iopub.status.idle": "2024-04-18T23:59:39.793800Z",
     "shell.execute_reply": "2024-04-18T23:59:39.793212Z",
     "shell.execute_reply.started": "2024-04-18T23:59:34.615266Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# AutoModelForCausalLM 是用于加载预训练的因果语言模型（如GPT系列）\n",
    "# 而 AutoTokenizer 是用于加载与这些模型匹配的分词器。\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 这行设置将模型加载到 GPU 设备上，以利用 GPU 的计算能力进行快速处理\n",
    "device = \"cuda:0\" \n",
    "\n",
    "# 加载了一个因果语言模型。\n",
    "# model_dir 是模型文件所在的目录。\n",
    "# torch_dtype=\"auto\" 自动选择最优的数据类型以平衡性能和精度。\n",
    "# device_map=\"auto\" 自动将模型的不同部分映射到可用的设备上。\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 加载与模型相匹配的分词器。分词器用于将文本转换成模型能够理解和处理的格式。\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1615f8f-d85d-405c-ae6a-35a09a287061",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-04-18T23:59:48.107370Z",
     "iopub.status.busy": "2024-04-18T23:59:48.107027Z",
     "iopub.status.idle": "2024-04-18T23:59:48.141278Z",
     "shell.execute_reply": "2024-04-18T23:59:48.140787Z",
     "shell.execute_reply.started": "2024-04-18T23:59:48.107349Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 加载与模型相匹配的分词器。分词器用于将文本转换成模型能够理解和处理的格式\n",
    "prompt = \"你好，请介绍下你自己。\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# 使用分词器的 apply_chat_template 方法将上面定义的消息列表转换为一个格式化的字符串，适合输入到模型中。\n",
    "# tokenize=False 表示此时不进行令牌化，add_generation_prompt=True 添加生成提示。\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# 将处理后的文本令牌化并转换为模型输入张量，然后将这些张量移至之前定义的设备（GPU）上。\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "338677b6-bd62-4b46-af8e-1768da62ce19",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-04-18T23:59:52.095569Z",
     "iopub.status.busy": "2024-04-18T23:59:52.095127Z",
     "iopub.status.idle": "2024-04-18T23:59:56.061694Z",
     "shell.execute_reply": "2024-04-18T23:59:56.061080Z",
     "shell.execute_reply.started": "2024-04-18T23:59:52.095535Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用模型的 generate 方法基于输入生成文本。\n",
    "# max_new_tokens=512 限制了生成的最大令牌数量，以控制输出长度。\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "# 从生成的令牌中提取新生成的部分，忽略输入部分的令牌。\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "# 将生成的令牌ID解码为人类可读的文本，并且跳过特殊令牌，如填充令牌或其他非文本内容的标记。\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b58ab1a-98c8-4c76-80b3-23129d12a260",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-04-18T23:59:58.685084Z",
     "iopub.status.busy": "2024-04-18T23:59:58.684750Z",
     "iopub.status.idle": "2024-04-18T23:59:58.688024Z",
     "shell.execute_reply": "2024-04-18T23:59:58.687556Z",
     "shell.execute_reply.started": "2024-04-18T23:59:58.685065Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您好！我是阿里云开发的语言模型，我叫通义千问。我是由阿里云自主研发的超大规模语言模型，具有自然语言处理、机器翻译、问答系统等能力，能够回答用户的问题，生成文字内容，创作文本故事，进行代码实现等多种任务。我的设计目标是为用户提供最准确、最快捷和最有用的答案，帮助用户在各种场景中更好地理解和应用自然语言技术。我在不断学习和进化，以提供更好的服务和体验。如果您有任何问题或需要了解更详细的信息，请随时告诉我，我会尽力为您提供支持和解答。\n"
     ]
    }
   ],
   "source": [
    "# 打印模型返回结果\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e191cdd-5da9-4cbd-88a9-41028cf33205",
   "metadata": {},
   "source": [
    "- 显存占用\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c305208-da02-4e2c-8016-ac1f47a3c995",
   "metadata": {},
   "source": [
    "而伴随着多轮对话不断进行，还会占用越累越多的显存。这里重启Jupyter kernel可以清空显存占用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26772523-015b-4c32-a10b-640003d530e6",
   "metadata": {},
   "source": [
    "- 优化对话形式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32157336-f43f-49ad-ac6b-fda0022eccb8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;可以看出，基于transformer库的大模型调用流程非常复杂，其中会涉及非常多的底层技术细节。这些底层技术细节对于大模型应用开发人员来说并不重要，对于开发人员来说，一种更友好的大模型调用方法是“一问一答”式对话，类似与OpenAI提供的GPT系列大模型调用流程，我们只需要编辑好输入模型的内容，然后观察模型回答即可："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61df100-4f2c-4e93-8b8b-11ace3e2e16a",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202404171718108.png\" alt=\"image-20240417171848068\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ab87ae-9528-4e18-89eb-70a6e70b41a3",
   "metadata": {},
   "source": [
    "这里我们重启Jupyter kernel，清空显存后继续进行实验："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "001dc982-6da5-49bf-80a5-b219949c06ca",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-19T00:01:46.380749Z",
     "iopub.status.busy": "2024-04-19T00:01:46.380394Z",
     "iopub.status.idle": "2024-04-19T00:01:49.494609Z",
     "shell.execute_reply": "2024-04-19T00:01:49.494033Z",
     "shell.execute_reply.started": "2024-04-19T00:01:46.380716Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 08:01:47,854 - modelscope - INFO - PyTorch version 2.1.2+cu121 Found.\n",
      "2024-04-19 08:01:47,857 - modelscope - INFO - TensorFlow version 2.14.0 Found.\n",
      "2024-04-19 08:01:47,857 - modelscope - INFO - Loading ast index from /mnt/workspace/.cache/modelscope/ast_indexer\n",
      "2024-04-19 08:01:47,884 - modelscope - INFO - Loading done! Current index file version is 1.13.3, with md5 55e7043102d017111a56be6e6d7a6a16 and a total number of 972 components indexed\n",
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "model_dir=snapshot_download(\"qwen/Qwen1.5-1.8B-Chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b8c1538-f243-4fca-94e4-495776027162",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T00:01:54.986372Z",
     "iopub.status.busy": "2024-04-19T00:01:54.985947Z",
     "iopub.status.idle": "2024-04-19T00:01:54.990336Z",
     "shell.execute_reply": "2024-04-19T00:01:54.989908Z",
     "shell.execute_reply.started": "2024-04-19T00:01:54.986351Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a650c3-c9b5-405f-977a-da88a7a7cc7a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-19T00:02:02.501711Z",
     "iopub.status.busy": "2024-04-19T00:02:02.501347Z",
     "iopub.status.idle": "2024-04-19T00:02:02.506389Z",
     "shell.execute_reply": "2024-04-19T00:02:02.505587Z",
     "shell.execute_reply.started": "2024-04-19T00:02:02.501691Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ChatModel(model_dir, messages):\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a013dc9-7ce5-4546-bfc7-5c29522fb3c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T00:02:05.032082Z",
     "iopub.status.busy": "2024-04-19T00:02:05.031758Z",
     "iopub.status.idle": "2024-04-19T00:02:05.034922Z",
     "shell.execute_reply": "2024-04-19T00:02:05.034345Z",
     "shell.execute_reply.started": "2024-04-19T00:02:05.032064Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"请问什么是机器学习？\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一位助人为乐的助手。\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add70b5f-463e-4715-80b2-2f180f932b71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T00:02:07.332775Z",
     "iopub.status.busy": "2024-04-19T00:02:07.332406Z",
     "iopub.status.idle": "2024-04-19T00:02:07.337947Z",
     "shell.execute_reply": "2024-04-19T00:02:07.337468Z",
     "shell.execute_reply.started": "2024-04-19T00:02:07.332757Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': '你是一位助人为乐的助手。'},\n",
       " {'role': 'user', 'content': '请问什么是机器学习？'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73c621e8-a7b4-4192-a6a6-174d42f3c1b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T00:02:14.403324Z",
     "iopub.status.busy": "2024-04-19T00:02:14.402970Z",
     "iopub.status.idle": "2024-04-19T00:02:24.798453Z",
     "shell.execute_reply": "2024-04-19T00:02:24.797850Z",
     "shell.execute_reply.started": "2024-04-19T00:02:14.403300Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "res = ChatModel(model_dir=model_dir, messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6abfe8e5-2c8b-40f1-b07c-c118947a92fd",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-04-19T00:02:27.495430Z",
     "iopub.status.busy": "2024-04-19T00:02:27.494982Z",
     "iopub.status.idle": "2024-04-19T00:02:33.401071Z",
     "shell.execute_reply": "2024-04-19T00:02:33.400547Z",
     "shell.execute_reply.started": "2024-04-19T00:02:27.495409Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "res = ChatModel(model_dir=model_dir, messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5237e30-d8f1-4888-ace4-541240a8ec93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T00:02:36.597870Z",
     "iopub.status.busy": "2024-04-19T00:02:36.597489Z",
     "iopub.status.idle": "2024-04-19T00:02:36.601353Z",
     "shell.execute_reply": "2024-04-19T00:02:36.600845Z",
     "shell.execute_reply.started": "2024-04-19T00:02:36.597851Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'机器学习是一种人工智能技术，它可以让计算机系统从数据中自动提取特征和模式，并基于这些特征和模式进行预测或决策。它的基本思想是让计算机通过训练大量数据来学习复杂的任务，并在未被预定义的情况下，自动发现数据中的规律和关系。\\n\\n具体来说，机器学习包括三个主要步骤：特征工程、模型选择和模型训练。首先，需要对输入数据进行预处理，例如清洗、归一化、特征提取等，以准备用于模型训练的数据集。然后，使用算法（如决策树、支持向量机、神经网络等）或其他模型，根据特征提取出具有潜在意义的特征。最后，利用已知的训练数据集来调整模型参数，使它能够更好地拟合训练数据，从而实现对新数据的预测或分类。\\n\\n机器学习广泛应用于各种领域，例如自然语言处理、计算机视觉、语音识别、推荐系统、医疗诊断、金融风控等。它可以帮助人们自动化分析大量复杂数据，提高工作效率，减少人为错误，从而推动各个领域的创新和发展。'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c109934-1b4b-476a-9374-02e36c214e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
